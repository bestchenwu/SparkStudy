2019-01-06 18:10:11 WARN  Utils:66 - Your hostname, sweet-ThinkPad-E420 resolves to a loopback address: 127.0.1.1; using 192.168.0.104 instead (on interface eth0)
2019-01-06 18:10:11 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-06 18:10:26 INFO  SparkContext:54 - Running Spark version 2.3.0
2019-01-06 18:10:26 INFO  SparkContext:54 - Submitted application: com.spark.scala.study.sql.HiveTest
2019-01-06 18:10:28 INFO  SecurityManager:54 - Changing view acls to: sweet
2019-01-06 18:10:28 INFO  SecurityManager:54 - Changing modify acls to: sweet
2019-01-06 18:10:28 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-06 18:10:28 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-06 18:10:28 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sweet); groups with view permissions: Set(); users  with modify permissions: Set(sweet); groups with modify permissions: Set()
2019-01-06 18:10:29 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 34621.
2019-01-06 18:10:29 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-06 18:10:29 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-06 18:10:29 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-06 18:10:29 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-06 18:10:29 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-62bc368e-0ed1-474b-8fab-a9cda0f777f6
2019-01-06 18:10:30 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2019-01-06 18:10:30 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-06 18:10:30 INFO  log:192 - Logging initialized @34164ms
2019-01-06 18:10:30 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-06 18:10:30 INFO  Server:414 - Started @34683ms
2019-01-06 18:10:31 INFO  AbstractConnector:278 - Started ServerConnector@273c947f{HTTP/1.1,[http/1.1]}{0.0.0.0:4052}
2019-01-06 18:10:31 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4052.
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73e132e0{/jobs,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@545f80bf{/jobs/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@66f66866{/jobs/job,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d666b41{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6594402a{/stages,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30f4b1a6{/stages/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@405325cf{/stages/stage,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/pool,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38600b{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@669d2b1b{/storage,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@721eb7df{/storage/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1ea9f009{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5d52e3ef{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5298dead{/environment,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@553f3b6e{/environment/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4c7a078{/executors,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e406694{/executors/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ab9b447{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@76f10035{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4f8caaf3{/static,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4163f1cd{/,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5fa05212{/api,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5f574cc2{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@680bddf5{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-06 18:10:31 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.104:4052
2019-01-06 18:10:31 INFO  SparkContext:54 - Added JAR file:/data/StudySpace/SparkStudy/target/spark-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://192.168.0.104:34621/jars/spark-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1546769431660
2019-01-06 18:10:32 INFO  StandaloneAppClient$ClientEndpoint:54 - Connecting to master spark://localhost:7077...
2019-01-06 18:10:32 INFO  TransportClientFactory:267 - Successfully created connection to localhost/127.0.0.1:7077 after 143 ms (0 ms spent in bootstraps)
2019-01-06 18:10:32 INFO  StandaloneSchedulerBackend:54 - Connected to Spark cluster with app ID app-20190106181032-0000
2019-01-06 18:10:32 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46635.
2019-01-06 18:10:32 INFO  NettyBlockTransferService:54 - Server created on 192.168.0.104:46635
2019-01-06 18:10:32 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-06 18:10:32 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.0.104, 46635, None)
2019-01-06 18:10:32 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.0.104:46635 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.104, 46635, None)
2019-01-06 18:10:32 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20190106181032-0000/0 on worker-20190106174008-192.168.0.104-46221 (192.168.0.104:46221) with 2 core(s)
2019-01-06 18:10:33 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20190106181032-0000/0 on hostPort 192.168.0.104:46221 with 2 core(s), 1024.0 MB RAM
2019-01-06 18:10:33 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.0.104, 46635, None)
2019-01-06 18:10:33 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.0.104, 46635, None)
2019-01-06 18:10:33 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20190106181032-0000/0 is now RUNNING
2019-01-06 18:10:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e8e8621{/metrics/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:34 INFO  StandaloneSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-01-06 18:10:34 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/data/StudySpace/SparkStudy/src/main/resources/script/spark-warehouse').
2019-01-06 18:10:34 INFO  SharedState:54 - Warehouse path is 'file:/data/StudySpace/SparkStudy/src/main/resources/script/spark-warehouse'.
2019-01-06 18:10:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1cbb3d3b{/SQL,null,AVAILABLE,@Spark}
2019-01-06 18:10:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@529cfee5{/SQL/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@415156bf{/SQL/execution,null,AVAILABLE,@Spark}
2019-01-06 18:10:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@393881f0{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-01-06 18:10:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1ecfcbc9{/static/sql,null,AVAILABLE,@Spark}
2019-01-06 18:10:37 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.104:40668) with ID 0
2019-01-06 18:10:37 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.0.104:45099 with 366.3 MB RAM, BlockManagerId(0, 192.168.0.104, 45099, None)
2019-01-06 18:10:38 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/data/StudySpace/SparkStudy/src/main/resources/script/src/main/resources/people.json;
	at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:397)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:340)
	at com.spark.scala.study.sql.HiveTest$.main(HiveTest.scala:11)
	at com.spark.scala.study.sql.HiveTest.main(HiveTest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2019-01-06 18:10:39 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-06 18:10:39 INFO  AbstractConnector:318 - Stopped Spark@273c947f{HTTP/1.1,[http/1.1]}{0.0.0.0:4052}
2019-01-06 18:10:39 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.0.104:4052
2019-01-06 18:10:39 INFO  StandaloneSchedulerBackend:54 - Shutting down all executors
2019-01-06 18:10:39 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Asking each executor to shut down
2019-01-06 18:10:39 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-06 18:10:39 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-06 18:10:39 INFO  BlockManager:54 - BlockManager stopped
2019-01-06 18:10:39 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-06 18:10:39 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-06 18:10:39 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-06 18:10:39 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-06 18:10:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-a81ddd97-ea60-40df-a781-7557d5f91855
2019-01-06 18:10:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-ec9385d5-e5d7-4cd1-a7da-f16129878c67
